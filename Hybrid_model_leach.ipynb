{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "import copy\n",
    "import random\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Libraries for mobolity\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "import clustering\n",
    "import clients\n",
    "import numpy\n",
    "from scipy.spatial.distance import cdist\n",
    "from mobility import gauss_markov, reference_point_group, \\\n",
    "    tvc, truncated_levy_walk, random_direction, random_waypoint, random_walk\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import Leach\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Clustering Library\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "import kmean_clustering\n",
    "\n",
    "#custom library\n",
    "import custom_model\n",
    "import server\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#os.environ[\"WANDB_API_KEY\"] = \"183c1a6a36cbdf0405f5baacb72690845ecc8573\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dataset\n",
    "## validating dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: cifar_data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: cifar_data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "\n",
      "trainset datatype: <class 'torchvision.datasets.cifar.CIFAR10'>\n",
      "testset datatype: <class 'torchvision.datasets.cifar.CIFAR10'>\n",
      "\n",
      "Label in cifar10: [0 1 2 3 4 5 6 7 8 9]\n",
      "('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='cifar_data', train=True,download=True,transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='cifar_data', train=False, download=True,transform=transform)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                                           batch_size=500,\n",
    "                                           shuffle=True)\n",
    "\n",
    "print(str(\"\\ntrainset datatype: \")+str((type(trainset))))\n",
    "print(str(\"testset datatype: \")+str((type(testset))))\n",
    "\n",
    "\n",
    "print(str(\"\\nLabel in cifar10: \")+str(np.unique([trainset[i][1] for i in range(len(trainset))])))\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print(classes)\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "#imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "#print(' '.join(f'{classes[labels[j]]:1s}' for j in range(16)))\n",
    "\n",
    "# # This is for iid data\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, \n",
    "#                                           batch_size=500,\n",
    "#                                           shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                        batch_size=100,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: \n",
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_client = 30\n",
    "n_leader = int(n_client * 0.1)\n",
    "chosen_prob = 0.8\n",
    "local_batch_size = 32\n",
    "local_epochs = 5\n",
    "\n",
    "epochs = 5\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD\n",
    "optimizer_conf = dict(\n",
    "    lr=0.001,\n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model = custom_model.Net().to(device)\n",
    "print(str(\"Model structure: \\n\")+str(model))\n",
    "\n",
    "server_list: List[server.Server] = []\n",
    "\n",
    "leaderStep = int(n_client/n_leader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobility\n",
    "## Assigning coordinates to nodes & creating clusters using Leach protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates generated: \n",
      "[[18.39839891 26.79646631]\n",
      " [95.03824448 61.86341421]\n",
      " [90.38934082 26.38757339]\n",
      " [53.34519328 56.74059427]\n",
      " [39.41817051 10.90634014]\n",
      " [28.42918166 56.92422588]\n",
      " [80.42768    61.58491742]\n",
      " [32.02827993 24.42435152]\n",
      " [85.68114019 41.30603091]\n",
      " [86.33075913 25.14261621]\n",
      " [76.18378956 65.86985846]\n",
      " [47.72839356 66.39393991]\n",
      " [20.25853068 91.03235363]\n",
      " [18.43027994 95.53617346]\n",
      " [35.96631085 12.61518217]\n",
      " [61.78414449  2.45425339]\n",
      " [76.19556261 69.10506403]\n",
      " [62.37692549  1.44829415]\n",
      " [38.21671773 20.38841057]\n",
      " [63.93959551 86.84645104]\n",
      " [47.15005807  2.66312755]\n",
      " [10.8449529  57.94976435]\n",
      " [73.43714486  5.15827358]\n",
      " [47.86458779 90.63239674]\n",
      " [52.63976174 89.99001038]\n",
      " [13.13148564 51.57567912]\n",
      " [56.58021295 76.97139824]\n",
      " [69.62869179 99.03848919]\n",
      " [49.14497882 78.8885162 ]\n",
      " [18.26780863 32.33673877]]\n",
      "\n",
      "Cluster Heads generated: \n",
      "[array([95.03824448, 61.86341421]), array([76.18378956, 65.86985846]), array([62.37692549,  1.44829415]), array([38.21671773, 20.38841057]), array([73.43714486,  5.15827358])]\n",
      "\n",
      "header_coord: \n",
      "[1, 10, 17, 18, 22]\n",
      "\n",
      "clusters generated: \n",
      "[4, 0, 4, 3, 4, 3, 1, 4, 0, 4, 1, 3, 3, 3, 4, 4, 1, 4, 4, 1, 4, 4, 4, 1, 1, 4, 1, 1, 1, 4]\n",
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "classes_pair = [(0, 1), (2, 3), (4, 5), (6, 7), (8, 9)]\n",
    "\n",
    "data_label = np.array(trainset.targets)\n",
    "\n",
    "nr_nodes = int(n_client)\n",
    "\n",
    "logger = logging.getLogger(\"vanet\")\n",
    "\n",
    "step = 0\n",
    "np.random.seed(0xffff)\n",
    "\n",
    "# simulation area (units)\n",
    "MAX_X, MAX_Y = 100, 100\n",
    "\n",
    "# max and min velocity\n",
    "MIN_V, MAX_V = 0.1, 1.\n",
    "\n",
    "# max waiting time\n",
    "MAX_WT = 100.\n",
    "\n",
    "# number of steps to ignore before start plotting\n",
    "STEPS_TO_IGNORE = 10000\n",
    "\n",
    "# set this to true if you want to calculate node contacts\n",
    "CALCULATE_CONTACTS = False\n",
    "\n",
    "# if calculating contacts, this is the range to be used\n",
    "# (if a distance(a,b) < RANGE, then there is a contact betwen a and b)\n",
    "RANGE = 1.\n",
    "\n",
    "#rw = random_walk(nr_nodes, dimensions=(MAX_X, MAX_Y))\n",
    "#rw = gauss_markov(nr_nodes, dimensions=(MAX_X, MAX_Y))\n",
    "#rw = truncated_levy_walk(nr_nodes, dimensions=(MAX_X, MAX_Y))\n",
    "rw = reference_point_group(nr_nodes, dimensions=(MAX_X, MAX_Y),velocity=(5.0, 10.0))\n",
    "\n",
    "cluster = clustering.Cluster(nr_nodes,\n",
    "                             step,MAX_X, \n",
    "                             MAX_Y,MIN_V, \n",
    "                             MAX_V,MAX_WT,\n",
    "                             STEPS_TO_IGNORE,\n",
    "                             CALCULATE_CONTACTS,\n",
    "                             RANGE,rw)\n",
    "\n",
    "coordinate = cluster.coord(rw)\n",
    "print(str(\"Coordinates generated: \\n\") + str(coordinate))\n",
    "\n",
    "leach = Leach.Leach(coordinate)\n",
    "\n",
    "node = leach.assign_energy(coordinate)\n",
    "\n",
    "array_header = leach.define_header(node,coordinate)\n",
    "\n",
    "print(str(\"\\nCluster Heads generated: \\n\")+str(array_header))\n",
    "\n",
    "headers = np.array(array_header)\n",
    "\n",
    "header_coord_index = []\n",
    "for i in range(len(coordinate)):\n",
    "    for j in range(len(headers)):\n",
    "        if (coordinate[i][0]) == (headers[j][0]):\n",
    "            header_coord_index.append(i)\n",
    "\n",
    "print(str(\"\\nheader_coord: \\n\")+str(header_coord_index))\n",
    "distance = []\n",
    "\n",
    "for i in range(len(array_header)):\n",
    "    for j in range(len(coordinate)):\n",
    "        dist = leach.calculate_distance(coordinate[j][0],\n",
    "                                  array_header[i][0],\n",
    "                                  coordinate[j][1],\n",
    "                                  array_header[i][1])\n",
    "        distance.append(dist)\n",
    "        \n",
    "newdistance = np.array_split(distance, len(array_header))\n",
    "#print(newdistance)\n",
    "\n",
    "clusters = leach.assign_cluster(newdistance)\n",
    "            \n",
    "print(str(\"\\nclusters generated: \\n\")+str(clusters))\n",
    "\n",
    "        \n",
    "leach.plot_cluster(clusters,coordinate)\n",
    "\n",
    "coordinate_cluster = [[] for _ in range(len(array_header))]\n",
    "\n",
    "for i in range(len(array_header)):\n",
    "    for j in range(len(clusters)):\n",
    "        if clusters[j] == i:\n",
    "            coordinate_cluster[i].append(coordinate[j])\n",
    "\n",
    "#print(coordinate_cluster[0])\n",
    "\n",
    "client_list = []\n",
    "\n",
    "for i in range(len(coordinate_cluster)):\n",
    "    client_list.append(len(coordinate_cluster[i]))\n",
    "    \n",
    "#print(client_list)\n",
    "\n",
    "cluster_number = len(array_header)\n",
    "\n",
    "max_value = numpy.max(clusters)\n",
    "\n",
    "cluster_coords = leach.initiaiseClusterCoordinates(len(array_header),clusters)\n",
    "\n",
    "cluster_coords = leach.fillingNoneValue(cluster_coords,cluster_number,n_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "servers = []\n",
    "\n",
    "for i in range(0,len(array_header)):\n",
    "    server_list = server.Server(\n",
    "        model=model,\n",
    "        loss=criteria,\n",
    "        optimizer=optimizer,\n",
    "        n_client=client_list[i],\n",
    "        #n_leader=n_leader,\n",
    "        chosen_prob=chosen_prob,\n",
    "        optimizer_conf=optimizer_conf,\n",
    "        local_batch_size=local_batch_size,\n",
    "        local_epochs=local_epochs\n",
    "    )\n",
    "    servers.append(server_list)\n",
    "    \n",
    "def initialiseClientCoordinates(best_k,coordinate,cluster_coords):\n",
    "    for i in range(int(best_k)):\n",
    "        for j in range(len(coordinate)):\n",
    "            for k in range(len(cluster_coords[i])):\n",
    "                #print(str(cluster_coords[i][j])+ str(' coord: ') + str(coordinate[k]))\n",
    "                if (cluster_coords[i][j] == k):\n",
    "                    servers[i].client_pool[j].setXCoordinate(coordinate[k][0])\n",
    "                    servers[i].client_pool[j].setYCoordinate(coordinate[k][1])\n",
    "\n",
    "        \n",
    "initialiseClientCoordinates(len(array_header),coordinate,cluster_coords)\n",
    "\n",
    "def showClient():\n",
    "    for i in range(int(best_k)):\n",
    "        for j in range(client_list[i]):\n",
    "            print(str('for server: ') + str(i) + str(' client ') + str(j) \n",
    "                  + str(' coordinates ') + str(servers[i].client_pool[j].actual_Xcoord) + str(', ') \n",
    "                  + str(servers[i].client_pool[j].actual_Ycoord))\n",
    "            \n",
    "#showClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  6.03it/s]\n",
      "9it [00:01,  6.28it/s]\n",
      "0it [00:00, ?it/s]\n",
      "5it [00:00,  5.60it/s]\n",
      "14it [00:03,  4.34it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # This is for iid data\n",
    "# for batch_idx, (batch_feature, batch_label) in enumerate(trainloader):\n",
    "#     server.client_pool[batch_idx].setData(list(zip(batch_feature, batch_label)))\n",
    "\n",
    "# This is for non-iid data                   \n",
    "chosen_counter = Counter()\n",
    "for i in range(len(servers)):\n",
    "    for _, client in tqdm(enumerate(servers[i].client_pool)):\n",
    "        # sample until we have a pair of class with insufficient client owning\n",
    "        class_pair = random.choice(classes_pair)\n",
    "        while chosen_counter[class_pair] == 100:\n",
    "            class_pair = random.choice(classes_pair)\n",
    "\n",
    "        chosen_counter[class_pair] += 1\n",
    "        first_class, second_class = class_pair\n",
    "        first_class_sample_idx = list(np.where(data_label == first_class)[0])\n",
    "        second_class_sample_idx = list(np.where(data_label == second_class)[0])\n",
    "        #first_class_sample_idx = list(first_class[0])\n",
    "        #second_class_sample_idx = list(scond_class[0])\n",
    "\n",
    "        client_first_class_sample_idx = random.sample(first_class_sample_idx, k=500)\n",
    "        client_second_class_sample_idx = random.sample(second_class_sample_idx, k=500)\n",
    "        client_data = []\n",
    "    \n",
    "        for i in range(500):\n",
    "            client_data.append(trainset[client_first_class_sample_idx[i]])\n",
    "            client_data.append(trainset[client_second_class_sample_idx[i]])\n",
    "\n",
    "        client.setData(client_data)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic nodes clustering using Leach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "[[ 15.42016394  24.5889661 ]\n",
      " [ 92.04006623  59.92374819]\n",
      " [ 87.71253018  25.12373056]\n",
      " [ 52.08954697  55.40837986]\n",
      " [ 37.96633874   8.06997205]\n",
      " [ 25.7659834   55.67266965]\n",
      " [ 78.1911557   60.556543  ]\n",
      " [ 30.89005269  22.93164603]\n",
      " [ 82.81116382  38.81293751]\n",
      " [ 83.39121971  23.48505698]\n",
      " [ 74.96751658  64.49096391]\n",
      " [ 44.89258719  63.84491559]\n",
      " [ 18.36837627  88.03840497]\n",
      " [ 16.10568532  92.59032022]\n",
      " [ 34.92539973  10.89828731]\n",
      " [183.08915312   5.39360755]\n",
      " [ 74.25688639  66.1069461 ]\n",
      " [184.9175541    6.37013474]\n",
      " [ 36.7856348   19.21080547]\n",
      " [ 60.98053555  84.56324793]\n",
      " [ 44.9964143    1.65125385]\n",
      " [  8.13873469  55.24177011]\n",
      " [ 70.58242515   3.67736338]\n",
      " [ 45.59829268  89.59628829]\n",
      " [ 49.96938177  87.24799239]\n",
      " [ 10.14893509  49.38968333]\n",
      " [ 55.29852647  75.66711779]\n",
      " [ 67.59667683  97.96202342]\n",
      " [ 47.14372197  77.88851541]\n",
      " [ 16.28739097  31.33654702]]\n",
      "[4, 0, 4, 3, 4, 3, 1, 4, 0, 4, 1, 3, 3, 3, 4, 2, 1, 2, 4, 1, 4, 4, 4, 1, 1, 4, 1, 1, 1, 4]\n",
      "Using matplotlib backend: Qt5Agg\n",
      "[[ 16.42012258  24.57987145]\n",
      " [ 91.64309055  60.84157732]\n",
      " [ 87.88272907  24.13832082]\n",
      " [ 52.77052418  56.14068447]\n",
      " [ 37.8019487    9.05636746]\n",
      " [ 24.94805967  56.24799631]\n",
      " [ 78.45197744  59.59115602]\n",
      " [ 29.91382147  22.71491436]\n",
      " [ 83.57071053  38.16248475]\n",
      " [ 83.59252663  22.50552876]\n",
      " [ 75.7457353   65.11895723]\n",
      " [ 45.78779588  64.29056275]\n",
      " [ 17.62458167  87.36999672]\n",
      " [ 15.96008962  91.60097605]\n",
      " [ 35.8927207   11.15184235]\n",
      " [185.11549107   3.18137915]\n",
      " [ 75.24376728  66.26839612]\n",
      " [182.95537606   4.10080502]\n",
      " [ 37.00002899  20.18755269]\n",
      " [ 61.95661421  84.34583023]\n",
      " [ 45.73378688   2.32674011]\n",
      " [  7.72038183  56.1500547 ]\n",
      " [ 69.91322517   4.42044574]\n",
      " [ 46.2526557   90.35246885]\n",
      " [ 50.65053437  87.98013387]\n",
      " [ 10.57951774  50.29223449]\n",
      " [ 54.78566401  74.808647  ]\n",
      " [ 68.59559657  97.99150768]\n",
      " [ 48.1180255   78.11375449]\n",
      " [ 16.83844207  30.50207551]]\n",
      "[4, 0, 4, 1, 4, 3, 1, 4, 0, 4, 1, 3, 3, 3, 4, 2, 1, 2, 4, 1, 4, 4, 4, 1, 1, 4, 1, 1, 1, 4]\n",
      "Using matplotlib backend: Qt5Agg\n",
      "[[ 16.31723254  25.57456419]\n",
      " [ 91.01957257  60.05976831]\n",
      " [ 86.90932296  24.36740717]\n",
      " [ 51.81891087  55.83338639]\n",
      " [ 36.93171628   8.56372607]\n",
      " [ 25.20357981  57.21480004]\n",
      " [ 79.05499361  58.79342706]\n",
      " [ 30.28041304  21.78453238]\n",
      " [ 84.37197494  38.76079517]\n",
      " [ 84.58254516  22.36459151]\n",
      " [ 76.74061167  65.22005626]\n",
      " [ 45.05823138  63.60665074]\n",
      " [ 18.62284738  87.42886576]\n",
      " [ 15.15370368  92.19236569]\n",
      " [ 36.64190325  10.48947878]\n",
      " [182.71644858   4.98265512]\n",
      " [ 75.14597614  67.26360308]\n",
      " [180.14462169   5.14945171]\n",
      " [ 36.21475905  19.56839914]\n",
      " [ 62.50011943  83.50642445]\n",
      " [ 45.67805436   1.32829437]\n",
      " [  8.62846102  56.5688532 ]\n",
      " [ 69.31434327   3.61960838]\n",
      " [ 45.25508324  90.2828327 ]\n",
      " [ 51.3583744   87.2737611 ]\n",
      " [ 10.70552139  51.28426426]\n",
      " [ 54.37817544  75.72185731]\n",
      " [ 67.75884009  97.44393249]\n",
      " [ 47.80717979  79.06421487]\n",
      " [ 17.63416853  31.10773168]]\n",
      "[4, 0, 4, 1, 4, 3, 1, 4, 0, 4, 1, 3, 3, 3, 4, 2, 1, 2, 4, 1, 4, 4, 4, 1, 1, 4, 1, 1, 1, 4]\n",
      "Using matplotlib backend: Qt5Agg\n",
      "[[ 16.97670791  24.82283802]\n",
      " [ 91.47598295  59.16999892]\n",
      " [ 86.1719228   23.69195102]\n",
      " [ 52.80935478  55.69547025]\n",
      " [ 37.08618698   9.55172344]\n",
      " [ 25.94198297  56.54044052]\n",
      " [ 79.96418052  59.20981529]\n",
      " [ 31.24737659  21.52961771]\n",
      " [ 83.40025917  38.99694862]\n",
      " [ 83.71392134  21.86911935]\n",
      " [ 76.15967084  64.40611046]\n",
      " [ 46.04777903  63.46244443]\n",
      " [ 17.76699641  86.91164325]\n",
      " [ 15.3801678   91.21834618]\n",
      " [ 37.07676469  11.38997615]\n",
      " [180.26634947   6.71384384]\n",
      " [ 74.54519612  68.06301747]\n",
      " [180.21949306   8.14851728]\n",
      " [ 37.00591688  18.9567871 ]\n",
      " [ 63.08434664  84.31801459]\n",
      " [ 46.39366149   2.02679736]\n",
      " [  9.33241911  55.85861169]\n",
      " [ 68.75808079   2.78860173]\n",
      " [ 44.74594874  91.14351967]\n",
      " [ 50.40027839  87.56020836]\n",
      " [  9.70623304  51.24654423]\n",
      " [ 53.60586034  75.08661768]\n",
      " [ 66.77058961  97.59677554]\n",
      " [ 47.0893968   79.76048177]\n",
      " [ 16.79042619  30.57098327]]\n",
      "[4, 0, 4, 1, 4, 3, 1, 4, 0, 4, 1, 3, 3, 3, 4, 2, 1, 2, 4, 1, 4, 4, 4, 1, 1, 4, 1, 1, 1, 4]\n",
      "Using matplotlib backend: Qt5Agg\n",
      "[[16.09706226 25.29846764]\n",
      " [92.17841085 59.88175383]\n",
      " [87.05529659 24.16062014]\n",
      " [52.29798622 54.83610874]\n",
      " [37.99834579  9.96156035]\n",
      " [26.76198536 55.96808041]\n",
      " [80.55813087 58.40531359]\n",
      " [31.44447893 22.51000063]\n",
      " [84.08018644 39.73022818]\n",
      " [82.78560913 21.49731774]\n",
      " [76.95191124 65.01631956]\n",
      " [46.21925475 62.47725608]\n",
      " [16.86731724 87.34819495]\n",
      " [15.90990874 92.06650568]\n",
      " [37.94630698 10.89611768]\n",
      " [57.94192569  1.22710561]\n",
      " [74.99242808 67.16859946]\n",
      " [58.23286177  1.70333844]\n",
      " [36.40156346 19.75350344]\n",
      " [62.18300038 83.88491539]\n",
      " [47.24944337  2.54413418]\n",
      " [ 8.70693185 56.63884606]\n",
      " [68.66020918  3.78380078]\n",
      " [43.81203682 90.78601649]\n",
      " [49.56248894 88.10620181]\n",
      " [ 8.73817313 50.99582525]\n",
      " [54.27855973 75.82653358]\n",
      " [65.87748866 97.95336816]\n",
      " [47.62894335 78.918526  ]\n",
      " [17.57824028 31.18689638]]\n",
      "[4, 0, 4, 3, 4, 3, 1, 4, 0, 4, 1, 3, 3, 3, 4, 4, 1, 4, 4, 1, 4, 4, 4, 1, 1, 4, 1, 1, 1, 4]\n",
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib\n",
    "#looping process\n",
    "\n",
    "coordinate_array = []\n",
    "best_ks = []\n",
    "cluster_array = []\n",
    "kmeans_array = []\n",
    "cluster_coords_array = []\n",
    "\n",
    "LABEL_COLOR_MAP = {0 : 'r',\n",
    "                   1 : 'g',\n",
    "                   2 : 'b',\n",
    "                   3 : 'y',\n",
    "                   4 : 'b'\n",
    "                   }\n",
    "\n",
    "#label_color = [LABEL_COLOR_MAP[l] for l in kmeans.labels_]\n",
    "\n",
    "%matplotlib  \n",
    "for i in range(epochs):\n",
    "    coordinate = cluster.coord(rw)\n",
    "    leach = Leach.Leach(coordinate)\n",
    "    new_coord = leach.speedRate(coordinate)\n",
    "    new_coord = np.array(new_coord)\n",
    "    print(new_coord)  \n",
    "    node = leach.assign_energy(new_coord)\n",
    "   \n",
    "    array_header = leach.define_header(node,new_coord)\n",
    "    distance = []\n",
    "    \n",
    "    for j in range(len(array_header)):\n",
    "        for k in range(len(new_coord)):\n",
    "            dist = leach.calculate_distance(new_coord[k][0],\n",
    "                                      array_header[j][0],\n",
    "                                      new_coord[k][1],\n",
    "                                      array_header[j][1])\n",
    "            distance.append(dist)      \n",
    "    newdistance = np.array_split(distance, len(array_header))\n",
    "    clusters = leach.assign_cluster(newdistance)\n",
    "            \n",
    "    print(clusters)  \n",
    "         \n",
    "    leach.plot_cluster(clusters,new_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Secret: 1923275859\n",
      "Shares: (30261, 3294432030), (88716, 5943086535)\n",
      "Reconstructed secret: 3294432030\n",
      "Client 0: Acc 0.5234375, Loss: 0.7233080882579088\n",
      "Overall acc: 0.5234375, overall_loss: 0.7233080882579088\n",
      "Original Secret: 2645221515\n",
      "Shares: (94849, 10469315525), (38849, 5849875525), (15846, 3952358055), (97406, 10680242455), (43608, 6242445435), (94567, 10446053345), (93218, 10334774335), (65641, 8059947605), (55327, 7209145745)\n",
      "Reconstructed secret: 2645221515\n",
      "Client 7: Acc 0.5224609375, Loss: 0.6914281006902456\n",
      "Client 4: Acc 0.50390625, Loss: 0.7342945728451014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c62a8ae9b4a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mservers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mservers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtotal_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\uni\\Stage\\FL\\server.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mclient\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchosen_clients\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m             \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Client {client.client_id}: Acc {client.accuracy}, Loss: {client.total_loss}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchosen_clients\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\uni\\Stage\\FL\\clients.py\u001b[0m in \u001b[0;36mupdate_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#wandb.init(project=\"fl\", name=\"CNN_CIFAR_10_noniid\")\n",
    "#print((servers[0].client_pool[0].data))\n",
    "aggregates = []\n",
    "\n",
    "    \n",
    "for i in range(epochs):\n",
    "    for j in range(len(servers)):\n",
    "        servers[j].aggregate()\n",
    "        servers[j].broadcast()\n",
    "        total_correct = 0\n",
    "        #with torch.no_grad():\n",
    "        for _, (test_feature, test_label) in enumerate(testloader):\n",
    "            test_feature = test_feature.to(device)\n",
    "            test_label = test_label.to(device)\n",
    "            y_pred = servers[j].model(test_feature)\n",
    "            y_pred_decode = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "            total_correct += y_pred_decode.eq(test_label).sum().item()\n",
    "            #print(str('total_correct ')+str(total_correct))\n",
    "        test_acc = total_correct / 10000\n",
    "\n",
    "        print(\"Overall acc: {}, overall_loss: {}\".format(servers[j].avg_acc, servers[j].avg_loss))\n",
    "        #wandb.log({\"acc\": servers[j].avg_acc, \"loss\": servers[j].avg_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
